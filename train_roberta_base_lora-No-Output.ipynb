{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27981a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf4f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed values for reproducibility\n",
    "seed = 42\n",
    "trainer_seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# some cudnn methods can be random even after fixing the seed\n",
    "# unless you tell it to be deterministic\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f0918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining variables, parameters and configs for the run\n",
    "\n",
    "run_on_kaggle = 1                                        # Set this to 1 if running on Kaggle, 0 if running on local machine\n",
    "\n",
    "# Setting up the directories and test data paths\n",
    "if run_on_kaggle:\n",
    "    test_data_path = \"./data/test_unlabelled.pkl\"\n",
    "else:\n",
    "    test_data_path = \"/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl\"\n",
    "\n",
    "# Defining LoRA configurations and other paths for the model\n",
    "config = dict()\n",
    "\n",
    "# Model configurations\n",
    "config['model_name'] = \"roberta-base\"                          # Base model name\n",
    "config['num_labels'] = 4                                       # Number of labels for the classification task\n",
    "\n",
    "# Tokenizer Configuration for the run\n",
    "config['dataset_name'] = 'ag_news'                              # Dataset name for the run\n",
    "config['tokenizer_model_name'] = \"roberta-base\"                # Tokenizer model name\n",
    "config['truncation'] = True                                    # Truncate the input sequences to the maximum length\n",
    "config['padding'] = \"max_length\"                               # Pad the input sequences to the maximum length\n",
    "config['max_length'] = 128                                     # Maximum length of the input sequences\n",
    "\n",
    "# LoRA Configuration for the run\n",
    "config['lora_rank'] = 4                                        # Rank of the LoRA layers\n",
    "config['lora_alpha'] = 8                                       # Alpha value for the LoRA layers\n",
    "config['lora_dropout'] = 0.6                                   # Dropout rate for the LoRA layers\n",
    "config['target_modules'] = ['query', 'value']                  # Target modules for the LoRA layers\n",
    "config['lora_bias'] = \"none\"                                   # Bias configuration for the LoRA layers\n",
    "config['task_type'] = TaskType.SEQ_CLS                         # Task type for the LoRA layers\n",
    "config['use_dora'] = True                                      # Use DORA (Dynamic LoRA) for the model\n",
    "config['use_rslora'] = True                                    # Use RSLORA (Recurrent LoRA) for the model\n",
    "config['lora_init_weights'] = \"gaussian\"                       # Initialization weights for the LoRA layers\n",
    "config['lora_rank_pattern'] = {}                               # Dictionary to store the rank pattern for LoRA layers. This was used to assign a higher rank for the higher layers\n",
    "config['lora_alpha_pattern'] = {}                              # Dictionary to store the alpha pattern for LoRA layers. This was used to assign a higher alpha for the higher layers\n",
    "\n",
    "# Training Hyperparameters and configurations for the run\n",
    "config['output_dir'] = './model/best-model'                    # Directory to save the model checkpoints\n",
    "config['eval_strategy'] = \"epoch\"                              # Evaluation strategy for the training\n",
    "config['save_strategy'] = \"epoch\"                              # Save strategy for the training\n",
    "config['learning_rate'] = 2e-4                                 # Learning rate for the training\n",
    "config['train_batch_size'] = 32                                # Batch size for training\n",
    "config['eval_batch_size'] = 64                                 # Batch size for evaluation\n",
    "config['num_train_epochs'] = 10                                # Number of training epochs\n",
    "config['weight_decay'] = 0.1                                   # Weight decay for the optimizer\n",
    "config['warmup_ratio'] = 0.1                                   # Warmup ratio (percentage of total steps) for the learning rate scheduler\n",
    "config['lr_scheduler_type'] = \"cosine\"                         # Learning rate scheduler type\n",
    "config['load_best_model_at_end'] = True                        # Load the best model at the end of training\n",
    "config['metric_for_best_model'] = \"eval_loss\"                  # Metric to use for selecting the best model\n",
    "config['greater_is_better'] = False                            # Whether a higher value of the metric is better or not\n",
    "config['label_smoothing_factor'] = 0.0                         # Label smoothing factor for the loss function\n",
    "config['seed'] = trainer_seed                                  # Seed for the training for reproducibility\n",
    "config['max_grad_norm'] = 1.0                                  # Maximum gradient norm for gradient clipping\n",
    "\n",
    "# Setting config for early stopping\n",
    "config['early_stopping_patience'] = 3                          # Number of evaluations with no improvement after which training will be stopped\n",
    "config['early_stopping_threshold'] = 0.01                      # Minimum change to qualify as an improvement for early stopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Device to use for training (GPU or CPU)\n",
    "config['device'] = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "print(\"Using device:\", config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdbe846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for loading the data and tokenizing the inputs\n",
    "\n",
    "# Function to tokenize the input data\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples['text'], truncation = config['truncation'], padding = config['padding'], max_length = config['max_length'])\n",
    "\n",
    "# Function to load the dataset and tokenize it\n",
    "def load_data():\n",
    "    # Load the dataset using the Hugging Face datasets library\n",
    "    dataset = load_dataset(config['dataset_name'])\n",
    "\n",
    "    # Load the tokenizer using the Hugging Face transformers library\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['tokenizer_model_name'])\n",
    "\n",
    "    # Map the tokenize_function to the dataset\n",
    "    tokenized_dataset = dataset.map(lambda x: tokenize_function(x, tokenizer), batched = True)\n",
    "    tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "    tokenized_dataset.set_format(\"torch\", columns = [\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    return tokenized_dataset, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70de4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to configure peft model for training\n",
    "def get_lora_config_model():\n",
    "    model =  AutoModelForSequenceClassification.from_pretrained(config['model_name'], num_labels = config['num_labels'])\n",
    "\n",
    "    # Configure LoRA with parameters from global config, targeting specific modules\n",
    "    # Includes options for DoRA and RsLoRA variants if specified\n",
    "    lora_config = LoraConfig(\n",
    "        r = config['lora_rank'],\n",
    "        lora_alpha = config['lora_alpha'],\n",
    "        target_modules = config['target_modules'],\n",
    "        lora_dropout = config['lora_dropout'],\n",
    "        bias = config['lora_bias'],\n",
    "        task_type = config['task_type'],\n",
    "        use_dora = config['use_dora'],\n",
    "        use_rslora = config['use_rslora'],\n",
    "        init_lora_weights = config['lora_init_weights'],\n",
    "        # rank_pattern = config['lora_rank_pattern'],\n",
    "        # alpha_pattern = config['lora_alpha_pattern']\n",
    "    )\n",
    "\n",
    "    # Convert model to PEFT model with LoRA adapters and move to target device\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.to(config['device'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce1aab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes evaluation metrics (accuracy, F1-score, and precision) for model predictions\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions), 'f1': f1_score(labels, predictions, average='weighted'), \"precision\": precision_score(labels, predictions, average='weighted')}\n",
    "\n",
    "\n",
    "def train_model(model, tokenized_dataset, tokenizer):\n",
    "    # Set up training arguments from the config dictionary\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = config['output_dir'],\n",
    "        eval_strategy = config['eval_strategy'],\n",
    "        save_strategy = config['save_strategy'],\n",
    "        learning_rate = config['learning_rate'],\n",
    "        per_device_train_batch_size = config['train_batch_size'],\n",
    "        per_device_eval_batch_size = config['eval_batch_size'],\n",
    "        num_train_epochs = config['num_train_epochs'],\n",
    "        weight_decay = config['weight_decay'],\n",
    "        lr_scheduler_type = config['lr_scheduler_type'],\n",
    "        warmup_ratio = config['warmup_ratio'],\n",
    "        load_best_model_at_end = config['load_best_model_at_end'],\n",
    "        metric_for_best_model = config['metric_for_best_model'],\n",
    "        greater_is_better = config['greater_is_better'],\n",
    "        seed = config['seed'],\n",
    "        # label_smoothing_factor = config['label_smoothing_factor'],\n",
    "        # max_grad_norm = config['max_grad_norm']\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer with model, data, tokenizer, metrics, and early stopping\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = tokenized_dataset[\"train\"],\n",
    "        eval_dataset = tokenized_dataset[\"test\"],\n",
    "        tokenizer = tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience = config['early_stopping_patience'], early_stopping_threshold = config['early_stopping_threshold'])],\n",
    "    )\n",
    "\n",
    "    # Start the training process\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset, model and starting the training process\n",
    "tokenized_dataset, tokenizer = load_data()\n",
    "model = get_lora_config_model()\n",
    "print(model.print_trainable_parameters())\n",
    "trainer = train_model(model, tokenized_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a18f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Final Evaluation Accuracy:\", eval_results[\"eval_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442312d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training loss and evaluation loss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Extracting the training loss and evaluation loss from the trainer\n",
    "# train_loss = trainer.state.log_history[0]['loss']\n",
    "# eval_loss = trainer.state.log_history[0]['eval_loss']\n",
    "# train_loss_values = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
    "# eval_loss_values = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n",
    "\n",
    "train_loss_values = []\n",
    "eval_loss_values = []\n",
    "eval_acc_values = []\n",
    "\n",
    "\n",
    "for i in range(len(trainer.state.log_history)):\n",
    "    log = trainer.state.log_history[i]\n",
    "    if 'eval_loss' in log:\n",
    "        eval_loss_values.append(log['eval_loss'])\n",
    "        eval_acc_values.append(log['eval_accuracy'])\n",
    "        train_loss_values.append(trainer.state.log_history[i-1].get('loss'))\n",
    "\n",
    "train_loss_values = train_loss_values[:-1]\n",
    "eval_loss_values = eval_loss_values[:-1]\n",
    "eval_acc_values = eval_acc_values[:-1]\n",
    "epoch_numbers = range(1, len(train_loss_values) + 1)\n",
    "\n",
    "# # Extracting the epoch numbers for the x-axis\n",
    "# epoch_numbers = [log['epoch'] for log in trainer.state.log_history if 'epoch' in log]\n",
    "\n",
    "# Plotting the training loss and evaluation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_numbers, train_loss_values, label='Training Loss', color='blue')\n",
    "plt.plot(epoch_numbers, eval_loss_values, label='Evaluation Loss', color='orange')  \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Evaluation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the evaluation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_numbers, eval_acc_values, label='Evaluation Accuracy', color='green')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Evaluation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078303cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "# merged_model = model.merge_and_unload()  # Merge LoRA weights with the base model weights\n",
    "\n",
    "# # Save the model\n",
    "# if not os.path.exists(config['output_dir']):\n",
    "#     os.makedirs(config['output_dir'])\n",
    "\n",
    "# merged_model.save_pretrained(config['output_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0fb76",
   "metadata": {},
   "source": [
    "### Generating submission on test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8abebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b01b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_results(model):\n",
    "    with open(test_data_path, 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "\n",
    "    test_dataset = Dataset.from_dict({\"text\": test_data[\"text\"]})\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['tokenizer_model_name'])\n",
    "    def tokenize_function(examples, tokenizer):\n",
    "        return tokenizer(examples['text'], truncation = config['truncation'], padding = config['padding'], max_length = config['max_length'])\n",
    "\n",
    "    tokenized_test_dataset = test_dataset.map(lambda x: tokenize_function(x, tokenizer), batched = True)\n",
    "    tokenized_test_dataset.set_format(\"torch\", columns = [\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "    test_dataloader = DataLoader(tokenized_test_dataset, batch_size = config['eval_batch_size'], shuffle = False)\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "         for batch in test_dataloader:\n",
    "             batch = {k: v.to(config['device']) for k, v in batch.items()}\n",
    "             outputs = model(**batch)\n",
    "             preds = torch.argmax(outputs.logits, dim=-1)\n",
    "             predictions.extend(preds.cpu().numpy()) \n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "    \"ID\": list(range(len(predictions))),   # ID ✅\n",
    "    \"label\": predictions\n",
    "    })\n",
    "    df.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"✅ Batched predictions complete. Saved to submission.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d5151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "dirs = os.listdir(config['output_dir'])\n",
    "print(dirs)\n",
    "dirs = sorted(dirs, key = lambda x: int(x.split('-')[1]))\n",
    "checkpoint = dirs[-1]\n",
    "checkpoint_path = os.path.join(config['output_dir'], checkpoint)\n",
    "trainer_state_json = os.path.join(checkpoint_path, 'trainer_state.json')\n",
    "with open(trainer_state_json, 'r') as f:\n",
    "    trainer_state = json.load(f)\n",
    "\n",
    "best_model_checkpoint = trainer_state['best_model_checkpoint']\n",
    "print(\"Best model checkpoint:\", best_model_checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(config['model_name'], num_labels = config['num_labels'])\n",
    "m = PeftModel.from_pretrained(model, best_model_checkpoint)\n",
    "m = m.merge_and_unload()\n",
    "m.to(config['device'])\n",
    "generate_test_results(m)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
